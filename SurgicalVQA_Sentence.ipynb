{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7S5UUf-SS4i",
        "outputId": "f4349c31-2cac-4a8d-96dc-76b9668b55aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'towards_foundation_model_surgery'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 71 (delta 36), reused 44 (delta 21), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (71/71), 6.74 MiB | 17.04 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mobarakol/towards_foundation_model_surgery.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2FglWpxT5VD",
        "outputId": "48cfc43a-c240-4122-b98b-3f040d499a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.64G/2.64G [00:29<00:00, 90.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/EndoVis-18-VQA.zip"
      ],
      "metadata": {
        "id": "86FmGruKURJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/towards_foundation_model_surgery/SurgicalGPT-V2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ol0A41HQbMAX",
        "outputId": "f3197841-e02b-4593-e60c-a52b9c22fcd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/towards_foundation_model_surgery/SurgicalGPT-V2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l281Z6t3VPw7",
        "outputId": "aaf9262d-aa0c-488a-9fd2-44e52998399d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv -f /content/EndoVis-18-VQA /content/towards_foundation_model_surgery/SurgicalGPT-V2/dataset"
      ],
      "metadata": {
        "id": "Wl7eMD7a6XKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_all = [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16]\n",
        "for sed in seq_all:\n",
        "    !mv /content/towards_foundation_model_surgery/SurgicalGPT-V2/dataset/EndoVis-18-VQA/seq_{sed}/vqa/Images /content/towards_foundation_model_surgery/SurgicalGPT-V2/dataset/EndoVis-18-VQA/seq_{sed}/left_frames"
      ],
      "metadata": {
        "id": "VtQAFNqhV6L7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace towards_foundation_model_surgery/SurgicalGPT-V2/model/EFGPT2Sentence.py with the following"
      ],
      "metadata": {
        "id": "mhWSyjygoPeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "\n",
        "from transformers import  VisualBertConfig, GPT2Config\n",
        "from transformers import VisualBertModel, GPT2Model, ViTModel, SwinModel\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "''' Early Fusion GPT with CNN/Transformers'''\n",
        "\n",
        "class EFVLEGPT2RS18Sentence(nn.Module):\n",
        "    def __init__(self, model_subver = 'v3', tokenizer_len=50258, vis_pos_emb = None):\n",
        "        super(EFVLEGPT2RS18Sentence, self).__init__()\n",
        "        '''\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "        '''\n",
        "\n",
        "        self.sub_ver = model_subver\n",
        "        self.vis_pos_emb = vis_pos_emb\n",
        "\n",
        "        ## image processing\n",
        "        self.img_feature_extractor = models.resnet18(pretrained=True)\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            new_fc = nn.Sequential(*list(self.img_feature_extractor.fc.children())[:-1])\n",
        "            self.img_feature_extractor.fc = new_fc\n",
        "        elif self.sub_ver == 'v2' or self.sub_ver =='v3':\n",
        "            self.img_feature_extractor = torch.nn.Sequential(*(list(self.img_feature_extractor.children())[:-2]))\n",
        "\n",
        "        ## Visual_embedding\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v2':\n",
        "            # visual bert embedding\n",
        "            VB_config = VisualBertConfig.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n",
        "            VB_config.visual_embedding_dim = 512\n",
        "            visualbert = VisualBertModel(config=VB_config)\n",
        "            self.visual_embedder = visualbert.embeddings.visual_projection\n",
        "        elif self.sub_ver == 'v1' or self.sub_ver =='v3':\n",
        "            self.visual_embedder = nn.Linear(512, 768)\n",
        "\n",
        "        ## word_embedding\n",
        "        # default GPT2 word embedding\n",
        "        gpt2configuration = GPT2Config()\n",
        "        word_embedder = GPT2Model(gpt2configuration)\n",
        "        word_embedder.resize_token_embeddings(tokenizer_len)\n",
        "        self.word_embedder = word_embedder.wte\n",
        "\n",
        "        ## GPT2 visual context aware decoder\n",
        "        self.VCAdecoder = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "\n",
        "    def forward(self, question, img, answer):\n",
        "\n",
        "        ## Debugging message to check if it's entering the forward\n",
        "        print('\\n Inside forward for EFGPT2Sentence Model \\n')\n",
        "\n",
        "\n",
        "        ## image encoder features\n",
        "        img_feature = img\n",
        "\n",
        "\n",
        "        if self.sub_ver == 'v0' or self.sub_ver =='v1':\n",
        "            img_feature = torch.unsqueeze(img_feature, dim=1)\n",
        "        if self.sub_ver == 'v2'or self.sub_ver =='v3':\n",
        "            img_feature = torch.flatten(img_feature, start_dim=2)\n",
        "            img_feature = img_feature.permute((0,2,1))\n",
        "\n",
        "\n",
        "        ## visual Embedding : id type 1, pos: zero / incremental\n",
        "        visual_embeds = self.visual_embedder(img_feature)\n",
        "        visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
        "        visual_attention_mask = visual_attention_mask.to(device)\n",
        "\n",
        "        ## question embedding:\n",
        "        question['input_ids'] = question['input_ids'].to(device)\n",
        "        question_embeds = self.word_embedder(question['input_ids'])\n",
        "        question_attention_mask = question['attention_mask'].to(device)\n",
        "\n",
        "        ## answer embedding\n",
        "        answer['input_ids'] = answer['input_ids'].to(device)\n",
        "        answer_embeds = self.word_embedder(answer['input_ids'])\n",
        "        answer_attention_mask = answer['attention_mask'].to(device)\n",
        "\n",
        "        ## token type and position id for question\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            question_id_type = torch.zeros(*question_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            question_position_id = torch.arange(0,question_embeds.size()[1])\n",
        "            question_position_id = torch.unsqueeze(question_position_id,0)\n",
        "            question_position_id = question_position_id.repeat(question_embeds.size()[0], 1)\n",
        "            question_position_id = question_position_id.to(device)\n",
        "            question_len = len(question_position_id[0])\n",
        "\n",
        "        ## token type and position id for vision\n",
        "        if self.vis_pos_emb == 'zeroes':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.zeros(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "        elif self.vis_pos_emb == 'pos':\n",
        "            visual_id_type = torch.ones(*visual_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            visual_position_id = torch.arange(0,visual_embeds.size()[1])\n",
        "            visual_position_id = torch.unsqueeze(visual_position_id,0)\n",
        "            visual_position_id = visual_position_id.repeat(visual_embeds.size()[0], 1)\n",
        "            visual_position_id += (question_len)\n",
        "            visual_position_id = visual_position_id.to(device)\n",
        "        visual_len = len(visual_position_id[0])\n",
        "\n",
        "        ## token type and position id for answer\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            answer_id_type = torch.zeros(*answer_embeds.size()[:-1], dtype=torch.long, device=device)\n",
        "            answer_position_id = torch.arange(0,answer_embeds.size()[1])\n",
        "            answer_position_id = torch.unsqueeze(answer_position_id,0)\n",
        "            answer_position_id = answer_position_id.repeat(answer_embeds.size()[0], 1)\n",
        "            answer_position_id += (question_len+visual_len)\n",
        "            answer_position_id = answer_position_id.to(device)\n",
        "\n",
        "        ## combine visual and question embeds\n",
        "        ## vision first\n",
        "        # inputs_embeds = torch.cat((visual_embeds, question_embeds), dim=1)\n",
        "        # attention_mask = torch.cat((visual_attention_mask, question_attention_mask), dim=1)\n",
        "\n",
        "        # if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "        #     token_type_ids = torch.cat((visual_id_type, question_id_type), dim=1)\n",
        "        #     position_ids = torch.cat((visual_position_id, question_position_id), dim=1)\n",
        "\n",
        "        ## question first\n",
        "        inputs_embeds = torch.cat((question_embeds, visual_embeds, answer_embeds), dim=1)\n",
        "        attention_mask = torch.cat((question_attention_mask, visual_attention_mask, answer_attention_mask), dim=1)\n",
        "\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            token_type_ids = torch.cat((question_id_type, visual_id_type, answer_id_type), dim=1)\n",
        "            position_ids = torch.cat((question_position_id, visual_position_id, answer_position_id), dim=1)\n",
        "\n",
        "\n",
        "        ## VCA_GPT2 decoder\n",
        "        if self.vis_pos_emb == 'zeroes' or self.vis_pos_emb == 'pos':\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, position_ids = position_ids, token_type_ids = token_type_ids)\n",
        "        else:\n",
        "            out = self.VCAdecoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "xtgP6z_MonyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Replace towards_foundation_model_surgery/SurgicalGPT-V2/train_SGPT_V2_Sentence.py with the following"
      ],
      "metadata": {
        "id": "UDVtZfZ3or7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import argparse\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# import numpy as np\n",
        "# from torch import nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data  import DataLoader\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from transformers import GPT2Tokenizer #, AdamW\n",
        "\n",
        "from utils import *\n",
        "from gpt2utils import generate_sample\n",
        "from dataloader.dataloaderGPT2Sentence import *\n",
        "from model.EFGPT2Sentence import EFVLEGPT2RS18Sentence\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "'''\n",
        "Seed randoms\n",
        "'''\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    for i, (_, visual_features, questions, answers) in enumerate(tqdm(train_dataloader),0):\n",
        "\n",
        "        # prepare questions and answers\n",
        "        question_list = []\n",
        "        answer_list = []\n",
        "        for question in questions: question_list.append(question)\n",
        "        for answer in answers: answer_list.append(answer)\n",
        "\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "            answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = visual_features.to(device)\n",
        "            visual_len = 80\n",
        "\n",
        "        # model forward(question, img, answer)\n",
        "        logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "        # only consider loss on reference summary just like seq2seq models\n",
        "        idx = args.question_len + visual_len\n",
        "        shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "        shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "        shift_labels = shift_labels.to(device)\n",
        "\n",
        "        loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.update(loss.item())\n",
        "\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "        # predict_token_number =  [50 - a for a in idx]\n",
        "        # losses.update(loss.item(), sum(predict_token_number))\n",
        "        # epoch_loss += loss.item()\n",
        "        # scheduler.step()  # Update learning rate schedule\n",
        "        # model.zero_grad()\n",
        "\n",
        "    print(\"Epoch: {}/{} Loss: {:.6f} AVG_Loss: {:.6f}\".format(epoch, args.epochs, total_loss.val, total_loss.avg))\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = AverageMeter()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (_, visual_features, questions, answers) in enumerate(tqdm(val_loader),0):\n",
        "\n",
        "            # prepare questions and answers\n",
        "            question_list = []\n",
        "            answer_list = []\n",
        "            for question in questions: question_list.append(question)\n",
        "            for answer in answers: answer_list.append(answer)\n",
        "\n",
        "            if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                question_inputs = tokenizer(question_list, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "                answer_inputs = tokenizer(answer_list, padding=\"max_length\",max_length= args.answer_len, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = visual_features.to(device)\n",
        "                visual_len = 80\n",
        "\n",
        "            # model forward(question, img, answer)\n",
        "            logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
        "\n",
        "\n",
        "            # only consider loss on reference summary just like seq2seq models\n",
        "            idx = args.question_len + visual_len\n",
        "            shift_logits = logits[..., idx:-1, :].contiguous()\n",
        "            shift_labels = answer_inputs['input_ids'][..., 1:].contiguous() # 1 because answer has '<|sep|>' in front\n",
        "\n",
        "            # copy for logits and labels for sentence decoding and blue-4 score calculation\n",
        "            logits_copy = logits.clone()\n",
        "            shift_labels_copy = shift_labels.clone()\n",
        "\n",
        "            # loss calculation\n",
        "            shift_labels = shift_labels.to(device)\n",
        "            loss = criterion(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "            total_loss.update(loss.item())\n",
        "\n",
        "            # references    - Ground truth answer\n",
        "            answer_GT_dec = tokenizer.batch_decode(shift_labels_copy, skip_special_tokens= True)\n",
        "            for answer_GT_dec_i in answer_GT_dec: references.append([answer_GT_dec_i.split()])\n",
        "            # print(references)\n",
        "\n",
        "            # Hypotheses - predicted answer\n",
        "            _, answer_Gen_id = torch.max(logits_copy, dim=2)\n",
        "            answer_Gen_dec = tokenizer.batch_decode(answer_Gen_id, skip_special_tokens= True)\n",
        "            for answer_Gen_dec_i in answer_Gen_dec: hypotheses.append(answer_Gen_dec_i.split())\n",
        "            # print(hypotheses)\n",
        "\n",
        "\n",
        "        # Calculate BLEU1~4\n",
        "        metrics = {}\n",
        "        metrics[\"Bleu_1\"] = corpus_bleu(references, hypotheses, weights=(1.00, 0.00, 0.00, 0.00))\n",
        "        metrics[\"Bleu_2\"] = corpus_bleu(references, hypotheses, weights=(0.50, 0.50, 0.00, 0.00))\n",
        "        metrics[\"Bleu_3\"] = corpus_bleu(references, hypotheses, weights=(0.33, 0.33, 0.33, 0.00))\n",
        "        metrics[\"Bleu_4\"] = corpus_bleu(references, hypotheses, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "        print(\"Epoch: {}/{} EVA LOSS: {:.6f} BLEU-1 {:.6f} BLEU2 {:.6f} BLEU3 {:.6f} BLEU-4 {:.6f}\".format\n",
        "          (epoch, args.epochs, total_loss.avg, metrics[\"Bleu_1\"],  metrics[\"Bleu_2\"],  metrics[\"Bleu_3\"],  metrics[\"Bleu_4\"]))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=50,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00005,                            help=' 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/efvlegpt2rs18/m18/v3_p_qf_',      help='m18/c80')\n",
        "    parser.add_argument('--dataset_type',   default= 'm18',                                         help='m18/c80')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--model_subver',   default= 'v3',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--answer_len',     default= 35,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= 'efvlegpt2rs18',                               help='efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--vis_pos_emb',    default= 'pos',                                         help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "    '''\n",
        "    EFVLEGPT2RS18Sentence:\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "    EFVLEGPT2SwinSentence:\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + GPT2 decoder\n",
        "    '''\n",
        "\n",
        "    print(args.model_ver, args.model_subver, args.vis_pos_emb, args.dataset_type, args.lr, args.checkpoint_dir)\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    # GPU or CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "    if args.dataset_type == 'm18':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer_length = len(tokenizer)\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "        # train_seq = [1, 2, 3, 5, 6, 7, 9, 10, 14, 15, 16]\n",
        "        # val_seq = [4, 11, 12]\n",
        "        folder_head = 'dataset/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Sentence/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = EndoVis18VQAGPTSentence(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = EndoVis18VQAGPTSentence(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    # elif args.dataset_type == 'c80':\n",
        "    #     '''\n",
        "    #     Train and test for cholec dataset\n",
        "    #     '''\n",
        "    #     # tokenizer\n",
        "    #     if args.tokenizer_ver == 'gpt2v1':\n",
        "    #         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    #         tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    #     # dataloader\n",
        "    #     train_seq = [1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
        "    #     val_seq = [5, 11, 12, 17, 19, 26, 27, 31]\n",
        "    #     folder_head = 'dataset/Cholec80-VQA/Classification/'\n",
        "    #     folder_tail = '/*.txt'\n",
        "\n",
        "    #     if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "    #         train_dataset = Cholec80VQAGPTClassification(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    #         train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    #         val_dataset = Cholec80VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "    #         val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "\n",
        "    # elif args.dataset_type == 'psi':\n",
        "    #     '''\n",
        "    #     Train and test for psi-ava-vqa dataset\n",
        "    #     '''\n",
        "    #     # tokenizer\n",
        "    #     if args.tokenizer_ver == 'btv2': tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    #     elif args.tokenizer_ver == 'btv3': tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "    #     elif args.tokenizer_ver == 'gpt2v1':\n",
        "    #         tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    #         tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    #     # dataloader\n",
        "    #     train_seq  =[\n",
        "    #                     \"dataset/PSI-AVA-VQA/Train/C1_location.txt\",\n",
        "    #                     \"dataset/PSI-AVA-VQA/Train/C3_phase.txt\",\n",
        "    #                     \"dataset/PSI-AVA-VQA/Train/C4_step.txt\"\n",
        "    #                 ]\n",
        "    #     val_seq    =[\n",
        "    #                     \"dataset/PSI-AVA-VQA/Val/C1_location.txt\",\n",
        "    #                     \"dataset/PS I-AVA-VQA/Val/C3_phase.txt\",\n",
        "    #                     \"dataset/PSI-AVA-VQA/Val/C4_step.txt\"\n",
        "    #                 ]\n",
        "\n",
        "    #     # dataloader\n",
        "    #     if args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "    #         train_dataset = PSIAVAVQAGPTClassification(train_seq, model_ver=args.model_ver)\n",
        "    #         train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "    #         val_dataset = PSIAVAVQAGPTClassification(val_seq, model_ver=args.model_ver)\n",
        "    #         val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    # if args.checkpoint is None:\n",
        "    #     if args.model_ver == 'efvlegpt2rs18':\n",
        "\n",
        "        # elif args.model_ver == 'efvlegpt2Swin':\n",
        "        #     model = EFVLEGPT2SwinClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "        # elif args.model_ver == 'efvlegpt2ViT':\n",
        "        #     model = EFVLEGPT2ViTClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "        model = EFVLEGPT2RS18Sentence(model_subver = args.model_subver, tokenizer_len=len(tokenizer), vis_pos_emb = args.vis_pos_emb)\n",
        "        print(model)\n",
        "        # optimizer\n",
        "        optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        # best_Acc = checkpoint['Acc']\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        final_args = checkpoint['final_args']\n",
        "        for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    # print(final_args)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "    # print(model)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = CrossEntropyLoss().to(device)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "            adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "        # train\n",
        "        train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        # validation\n",
        "        metrics = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "        if metrics[\"Bleu_4\"] >= best_results[0]:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "            best_results[0] = metrics[\"Bleu_4\"]\n",
        "            best_epoch[0] = epoch\n",
        "            # print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
        "            save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0])\n",
        "\n",
        "        else:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))"
      ],
      "metadata": {
        "id": "HzQaZmj0o4UJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/towards_foundation_model_surgery/SurgicalGPT-V2/\n",
        "import os\n",
        "os.makedirs('checkpoints/efvlegpt2Swin', exist_ok=True)\n",
        "!python train_SGPT_V2_Sentence.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2Swin/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2Swin' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'\\\n",
        "--batch_size=40\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HPhY6ngUfgX",
        "outputId": "cccd4f0a-0abe-4d57-cc42-8586352299b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/towards_foundation_model_surgery/SurgicalGPT-V2\n",
            "2023-11-01 15:44:32.515990: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-01 15:44:32.516047: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-01 15:44:32.516089: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-01 15:44:33.647100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "efvlegpt2Swin v1 zeroes m18 1e-05 checkpoints/efvlegpt2Swin/m18_v1_z_qf_\n",
            "device = cuda\n",
            "Total files: 1560 | Total question: 10574\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Total files: 447 | Total question: 3216\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "EFVLEGPT2RS18Sentence(\n",
            "  (img_feature_extractor): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Sequential()\n",
            "  )\n",
            "  (visual_embedder): Linear(in_features=512, out_features=768, bias=True)\n",
            "  (word_embedder): Embedding(50257, 768)\n",
            "  (VCAdecoder): GPT2LMHeadModel(\n",
            "    (transformer): GPT2Model(\n",
            "      (wte): Embedding(50257, 768)\n",
            "      (wpe): Embedding(1024, 768)\n",
            "      (drop): Dropout(p=0.1, inplace=False)\n",
            "      (h): ModuleList(\n",
            "        (0-11): 12 x GPT2Block(\n",
            "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (attn): GPT2Attention(\n",
            "            (c_attn): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (mlp): GPT2MLP(\n",
            "            (c_fc): Conv1D()\n",
            "            (c_proj): Conv1D()\n",
            "            (act): NewGELUActivation()\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  )\n",
            ")\n",
            "model params:  174607680\n",
            "  0% 0/265 [00:00<?, ?it/s]\n",
            " Inside forward for EFGPT2Sentence Model \n",
            "\n",
            "  0% 0/265 [00:22<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/towards_foundation_model_surgery/SurgicalGPT-V2/train_SGPT_V2_Sentence.py\", line 346, in <module>\n",
            "    train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
            "  File \"/content/towards_foundation_model_surgery/SurgicalGPT-V2/train_SGPT_V2_Sentence.py\", line 70, in train\n",
            "    logits = model(question_inputs, visual_features, answer_inputs)[0]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/towards_foundation_model_surgery/SurgicalGPT-V2/model/EFGPT2Sentence.py\", line 68, in forward\n",
            "    img_feature = torch.unsqueeze(img_feature, dim=1)\n",
            "TypeError: unsqueeze(): argument 'input' (position 1) must be Tensor, not BatchFeature\n"
          ]
        }
      ]
    }
  ]
}