{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pU4iPZR0ucs",
        "outputId": "6dc7ed44-8061-4f97-c3a4-5bc8967ac45f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'SurgicalGPT'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 89 (delta 42), reused 82 (delta 35), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (89/89), 749.26 KiB | 1.60 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/lalithjets/SurgicalGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7wl6PSm0xF9",
        "outputId": "324d823a-10ac-4f75-a7fa-a2a2177d39c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/cli.py:121: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I\n",
            "To: /content/EndoVis-18-VQA.zip\n",
            "100% 2.64G/2.64G [00:37<00:00, 70.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading the VQA EndoVis18 Dataset\n",
        "!gdown --id 1K5YnSPMPvn2x1gtRAw2ZfxIqoIo2DX3I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cXtlX3Kb02w4"
      },
      "outputs": [],
      "source": [
        "# Unzipping the VQA EndoVis18 Dataset\n",
        "!unzip EndoVis-18-VQA.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWZYckbo1Wuv"
      },
      "outputs": [],
      "source": [
        "# Installing required dependencies\n",
        "!pip install transformers\n",
        "!pip install utils\n",
        "!pip install dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxRZLkuu3sCM"
      },
      "outputs": [],
      "source": [
        "# This code should be used to replace SurgicalGPT/train.py\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from lib2to3.pytree import convert\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.utils.data  import DataLoader\n",
        "from transformers import BertTokenizer, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from utils import *\n",
        "from dataloaders.dataloaderClassification import *\n",
        "from dataloaders.dataloaderGPT2Classification import *\n",
        "from models.VisualBertClassification import VisualBertClassification\n",
        "from models.VisualBertResMLPClassification import VisualBertResMLPClassification\n",
        "# from models.ViReVisualBertClassification import ViReVisualBertClassification\n",
        "from models.EFGPT2Classification import EFVLEGPT2RS18Classification, EFVLEGPT2SwinClassification, EFVLEGPT2ViTClassification\n",
        "# from models.EFGPT2GCVITClassification import EFGPT2GCVITClassification\n",
        "# from models.EFViLGPT2Classification import ViLGPT2VQA\n",
        "# from models.LFGPT2Classification import GPT2RS18Classification, GPT2ViTClassification, GPT2SwinClassification, BioGPT2RS18Classification\n",
        "# from models.ViReGPT2Classification import EFGPT2RS18GRClassification, EFVLEGPT2SwinGRClassification\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "\n",
        "class config_emb:\n",
        "    visual_embedding_dim = 2048\n",
        "    vocab_size = 30522\n",
        "    type_vocab_size = 2\n",
        "    pad_token_id = 1\n",
        "    hidden_size = 768\n",
        "    max_position_embeddings = 512\n",
        "    layer_norm_eps = 1e-12\n",
        "    hidden_dropout_prob = 0.1\n",
        "    special_visual_initialize = True\n",
        "\n",
        "'''\n",
        "EndoVis18 classification dataloader for VB transfomers\n",
        "'''\n",
        "class EndoVis18VQAGPTClassification(Dataset):\n",
        "    '''\n",
        "    \tseq: train_seq  = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "    \t     val_seq    = [1, 5, 16]\n",
        "    \tfolder_head     = 'dataset/EndoVis-18-VQA/seq_'\n",
        "    \tfolder_tail     = '/vqa/Classification/*.txt'\n",
        "    '''\n",
        "    def __init__(self, seq, folder_head, folder_tail, model_ver = None, transform=None):\n",
        "\n",
        "        if model_ver == \"efvlegpt2ViT\":\n",
        "            self.transform = None\n",
        "            self.image_processor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')\n",
        "        elif model_ver == \"efvlegpt2Swin\":\n",
        "            self.transform = None\n",
        "            self.image_processor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
        "        elif transform:\n",
        "            self.transform = transform\n",
        "        else:\n",
        "            self.transform = transforms.Compose([\n",
        "                                    transforms.Resize((300,256)),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "                                    ])\n",
        "        print(\"I worked \")\n",
        "\n",
        "\n",
        "        # files, question and answers\n",
        "        filenames = []\n",
        "        for curr_seq in seq: filenames = filenames + glob.glob(folder_head + str(curr_seq) + folder_tail)\n",
        "        self.vqas = []\n",
        "        for file in filenames:\n",
        "            file_data = open(file, \"r\")\n",
        "            lines = [line.strip(\"\\n\") for line in file_data if line != \"\\n\"]\n",
        "            file_data.close()\n",
        "            for line in lines: self.vqas.append([file, line])\n",
        "        print('Total files: %d | Total question: %.d' %(len(filenames), len(self.vqas)))\n",
        "\n",
        "        # Labels\n",
        "        self.labels = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                        'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                        'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                        'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.vqas)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        loc = self.vqas[idx][0].split('/')\n",
        "        # img\n",
        "        img_loc = os.path.join('/' + loc[1],loc[2],loc[3], 'left_frames',loc[-1].split('_')[0]+'.png')\n",
        "        if self.transform:\n",
        "            img = Image.open(img_loc)\n",
        "            img = self.transform(img)\n",
        "        else:\n",
        "            img = self.image_processor(Image.open(img_loc), return_tensors=\"pt\")\n",
        "\n",
        "        # question and answer\n",
        "        question = self.vqas[idx][1].split('|')[0]\n",
        "        label = self.labels.index(str(self.vqas[idx][1].split('|')[1]))\n",
        "\n",
        "        return os.path.join(loc[1],loc[2],loc[3], 'left_frames',loc[-1].split('_')[0]+'.png'), img, question, label\n",
        "\n",
        "'''\n",
        "Seed randoms\n",
        "'''\n",
        "def seed_everything(seed=27):\n",
        "    '''\n",
        "    Set random seed for reproducible experiments\n",
        "    Inputs: seed number\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def train(args, train_dataloader, model, criterion, optimizer, epoch, tokenizer, device):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "    train_dataloader_with_progress = tqdm(train_dataloader, total=len(train_dataloader), desc=\"Training\")\n",
        "\n",
        "\n",
        "    for i, (_, v_f, q, labels) in enumerate(train_dataloader,0):\n",
        "        # print('train')\n",
        "        # prepare questions\n",
        "        questions = []\n",
        "        for question in q: questions.append(question)\n",
        "\n",
        "        if args.model_ver == 'vb' or args.model_ver == 'vbrm':\n",
        "\n",
        "            inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
        "\n",
        "        elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            # inputs = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "            inputs = tokenizer(questions, padding=\"max_length\",max_length= args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "        # Visual features\n",
        "        if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "            # or args.model_ver == 'gpt2ViT' or args.model_ver == \"gpt2Swin\" \\\n",
        "            # or args.model_ver == \"efvlegpt2Swingr\" \\\n",
        "\n",
        "            visual_features = v_f\n",
        "            visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "        else:\n",
        "            visual_features = v_f.to(device)\n",
        "\n",
        "        # labels\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # model forward pass\n",
        "        outputs = model(inputs, visual_features)\n",
        "\n",
        "        # loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "        label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "        label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "        label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "    train_dataloader_with_progress.close()\n",
        "    # loss and acc\n",
        "    acc, c_acc = calc_acc(label_true, label_pred), calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "    print('Train: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "    return acc\n",
        "\n",
        "\n",
        "def validate(args, val_loader, model, criterion, epoch, tokenizer, device, save_output = False):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    label_true = None\n",
        "    label_pred = None\n",
        "    label_score = None\n",
        "    file_names = list()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (file_name, v_f, q, labels) in enumerate(val_loader,0):\n",
        "\n",
        "            # prepare questions\n",
        "            questions = []\n",
        "            for question in q: questions.append(question)\n",
        "\n",
        "            if args.model_ver == 'vb' or args.model_ver == 'vbrm':\n",
        "                # or args.model_ver == 'vrvb'\n",
        "\n",
        "                inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"max_length\", max_length=args.question_len)\n",
        "\n",
        "            elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "                # inputs = tokenizer(questions, padding=True, truncation=True, return_tensors=\"pt\",)\n",
        "                inputs = tokenizer(questions, padding=\"max_length\",max_length=args.question_len, return_tensors=\"pt\")\n",
        "\n",
        "            # GPU / CPU\n",
        "            # Visual features\n",
        "            if args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "                visual_features = v_f\n",
        "                visual_features['pixel_values'] = torch.squeeze(visual_features['pixel_values'],1)\n",
        "            else:\n",
        "                visual_features = v_f.to(device)\n",
        "\n",
        "            # label\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # model forward pass\n",
        "            outputs = model(inputs, visual_features)\n",
        "\n",
        "            # loss\n",
        "            loss = criterion(outputs,labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            scores, predicted = torch.max(F.softmax(outputs, dim=1).data, 1)\n",
        "            label_true = labels.data.cpu() if label_true == None else torch.cat((label_true, labels.data.cpu()), 0)\n",
        "            label_pred = predicted.data.cpu() if label_pred == None else torch.cat((label_pred, predicted.data.cpu()), 0)\n",
        "            label_score = scores.data.cpu() if label_score == None else torch.cat((label_score, scores.data.cpu()), 0)\n",
        "            for f in file_name: file_names.append(f)\n",
        "\n",
        "    acc = calc_acc(label_true, label_pred)\n",
        "    c_acc = 0.0\n",
        "    # c_acc = calc_classwise_acc(label_true, label_pred)\n",
        "    precision, recall, fscore = calc_precision_recall_fscore(label_true, label_pred)\n",
        "\n",
        "    print('Test: epoch: %d loss: %.6f | Acc: %.6f | Precision: %.6f | Recall: %.6f | FScore: %.6f' %(epoch, total_loss, acc, precision, recall, fscore))\n",
        "\n",
        "    if save_output:\n",
        "        '''\n",
        "            Saving predictions\n",
        "        '''\n",
        "        if os.path.exists(args.checkpoint_dir + 'text_files') == False:\n",
        "            os.mkdir(args.checkpoint_dir + 'text_files' )\n",
        "        file1 = open(args.checkpoint_dir + 'text_files/labels.txt', 'w')\n",
        "        file1.write(str(label_true))\n",
        "        file1.close()\n",
        "\n",
        "        file1 = open(args.checkpoint_dir + 'text_files/predictions.txt', 'w')\n",
        "        file1.write(str(label_pred))\n",
        "        file1.close()\n",
        "\n",
        "        if args.dataset_type == 'med_vqa':\n",
        "            if args.dataset_cat == 'cat1':\n",
        "                convert_arr = ['cta - ct angiography', 'no', 'us - ultrasound', 'xr - plain film', 'noncontrast', 'yes', 't2', 'ct w/contrast (iv)', 'mr - flair', 'mammograph', 'ct with iv contrast',\n",
        "                            'gi and iv', 't1', 'mr - t2 weighted', 'mr - t1w w/gadolinium', 'contrast', 'iv', 'an - angiogram', 'mra - mr angiography/venography', 'nm - nuclear medicine', 'mr - dwi diffusion weighted',\n",
        "                            'ct - gi & iv contrast', 'ct noncontrast', 'mr - other pulse seq.', 'ct with gi and iv contrast', 'flair', 'mr - t1w w/gd (fat suppressed)', 'ugi - upper gi', 'mr - adc map (app diff coeff)',\n",
        "                            'bas - barium swallow', 'pet - positron emission', 'mr - pdw proton density', 'mr - t1w - noncontrast', 'be - barium enema', 'us-d - doppler ultrasound', 'mr - stir', 'mr - flair w/gd',\n",
        "                            'ct with gi contrast', 'venogram', 'mr t2* gradient,gre,mpgr,swan,swi', 'mr - fiesta', 'ct - myelogram', 'gi', 'sbft - small bowel', 'pet-ct fusion']\n",
        "            elif args.dataset_cat == 'cat2':\n",
        "                convert_arr = ['axial', 'longitudinal', 'coronal', 'lateral', 'ap', 'sagittal', 'mammo - mlo', 'pa', 'mammo - cc', 'transverse', 'mammo - mag cc', 'frontal', 'oblique', '3d reconstruction', 'decubitus', 'mammo - xcc']\n",
        "            else:\n",
        "                convert_arr = ['lung, mediastinum, pleura', 'skull and contents', 'genitourinary', 'spine and contents', 'musculoskeletal', 'heart and great vessels', 'vascular and lymphatic', 'gastrointestinal', 'face, sinuses, and neck', 'breast']\n",
        "        elif args.dataset_type == 'c80':\n",
        "            convert_arr = ['no', 'calot triangle dissection', 'yes', '1', '2', 'gallbladder dissection',\n",
        "                            'clipping cutting', 'gallbladder retraction', '0', 'cleaning coagulation',\n",
        "                            'gallbladder packaging', 'preparation', '3']\n",
        "        elif args.dataset_type == 'm18':\n",
        "            convert_arr = ['kidney', 'Idle', 'Grasping', 'Retraction', 'Tissue_Manipulation',\n",
        "                            'Tool_Manipulation', 'Cutting', 'Cauterization', 'Suction',\n",
        "                            'Looping', 'Suturing', 'Clipping', 'Staple', 'Ultrasound_Sensing',\n",
        "                            'left-top', 'right-top', 'left-bottom', 'right-bottom']\n",
        "\n",
        "        df = pd.DataFrame(columns=[\"Img\", \"Ground Truth\", \"Prediction\"])\n",
        "        for i in range(len(label_true)):\n",
        "            df = df.append({'Img': file_names[i], 'Ground Truth': convert_arr[label_true[i]], 'Prediction': convert_arr[label_pred[i]]}, ignore_index=True)\n",
        "\n",
        "        df.to_csv(args.checkpoint_dir + args.checkpoint_dir.split('/')[1] + '_' + args.checkpoint_dir.split('/')[2] + '_eval.csv')\n",
        "\n",
        "    return (acc, c_acc, precision, recall, fscore)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='VisualQuestionAnswerClassification')\n",
        "\n",
        "    # VB Model parameters\n",
        "    parser.add_argument('--emb_dim',        type=int,   default=300,                                help='dimension of word embeddings.')\n",
        "    parser.add_argument('--n_heads',        type=int,   default=8,                                  help='Multi-head attention.')\n",
        "    parser.add_argument('--dropout',        type=float, default=0.1,                                help='dropout')\n",
        "    parser.add_argument('--encoder_layers', type=int,   default=6,                                  help='the number of layers of encoder in Transformer.')\n",
        "\n",
        "    # Training parameters\n",
        "    parser.add_argument('--epochs',         type=int,   default=80,                                 help='number of epochs to train for (if early stopping is not triggered).') #80, 26\n",
        "    parser.add_argument('--batch_size',     type=int,   default=64,                                 help='batch_size')\n",
        "    parser.add_argument('--workers',        type=int,   default=1,                                  help='for data-loading; right now, only 1 works with h5pys.')\n",
        "    parser.add_argument('--print_freq',     type=int,   default=100,                                help='print training/validation stats every __ batches.')\n",
        "\n",
        "    # existing checkpoint\n",
        "    parser.add_argument('--checkpoint',     default=None,                                           help='path to checkpoint, None if none.')\n",
        "\n",
        "    parser.add_argument('--lr',             type=float, default=0.00001,                           help='0.000005, 0.00001, 0.000005')\n",
        "    parser.add_argument('--checkpoint_dir', default= 'checkpoints/clf_biogpt2rs18/c80/',            help='med_vqa_c$version$/m18/c80/m18_vid$temporal_size$/c80_vid$temporal_size$') #clf_v1_2_1x1/med_vqa_c3\n",
        "    parser.add_argument('--dataset_type',   default= None,                                          help='med_vqa/m18/c80/m18_vid/c80_vid')\n",
        "    parser.add_argument('--dataset_cat',    default= 'cat1',                                        help='cat1/cat2/cat3')\n",
        "    parser.add_argument('--tokenizer_ver',  default= 'gpt2v1',                                      help='btv2/btv3/gpt2v1')\n",
        "    parser.add_argument('--question_len',   default= 25,                                            help='25')\n",
        "    parser.add_argument('--model_ver',      default= None,                                          help='vb/vbrm/efvlegpt2rs18/efvlegpt2Swin/\"')  #vrvb/gpt2rs18/gpt2ViT/gpt2Swin/biogpt2rs18/vilgpt2vqa/efgpt2rs18gr/efvlegpt2Swingr\n",
        "    parser.add_argument('--model_subver',   default= 'v0',                                          help='V0,v1/v2/v3/v4')\n",
        "    parser.add_argument('--vis_pos_emb',   default= None,                                           help='None, zeroes, pos')\n",
        "    parser.add_argument('--patch_size',     default= 5,                                             help='1/2/3/4/5')\n",
        "\n",
        "    parser.add_argument('--num_class',      default= 2,                                             help='25')\n",
        "    # parser.add_argument('--temporal_size',  default= 1,                                             help='1/2/3/4/5')\n",
        "    parser.add_argument('--validate',       default=False,                                          help='When only validation required False/True')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "\n",
        "    '''\n",
        "    EFVLEGPT2RS18Classification:\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + from nn.linear    + GPT2 decoder\n",
        "        v2: visual embedding : visual patches + embedding form VB + GPT2 decoder\n",
        "        v3: visual embedding : visual patches + from nn.linear    + GPT2 decoder\n",
        "    EFVLEGPT2SwinClassification:\n",
        "        v0: visual embedding : Default patch1 + embedding form VB + GPT2 decoder\n",
        "        v1: visual embedding : Default patch1 + GPT2 decoder\n",
        "    '''\n",
        "\n",
        "    print(args.model_ver, args.model_subver, args.vis_pos_emb, args.dataset_type, args.dataset_cat, args.lr, args.checkpoint_dir)\n",
        "    # load checkpoint, these parameters can't be modified\n",
        "    final_args = {\"emb_dim\": args.emb_dim, \"n_heads\": args.n_heads, \"dropout\": args.dropout, \"encoder_layers\": args.encoder_layers}\n",
        "\n",
        "    seed_everything()\n",
        "\n",
        "    # GPU or CPU\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "    cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "    print('device =', device)\n",
        "\n",
        "    # best model initialize\n",
        "    start_epoch = 1\n",
        "    best_epoch = [0]\n",
        "    best_results = [0.0]\n",
        "    epochs_since_improvement = 0\n",
        "\n",
        "\n",
        "    # dataset\n",
        "    if args.dataset_type == 'med_vqa':\n",
        "        '''\n",
        "        Train and test dataloader for MED_VQA\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'btv2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-medvqa/')\n",
        "        elif args.tokenizer_ver == 'btv3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-medvqa/', do_lower_case=True)\n",
        "        elif args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # data location\n",
        "        train_folder = 'dataset/VQA-Med/ImageClef-2019-VQA-Med-Training/'\n",
        "        val_folder = 'dataset/VQA-Med/ImageClef-2019-VQA-Med-Validation/'\n",
        "        train_img_folder = 'train_images/'\n",
        "        val_img_folder = 'val_images/'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'vb' or args.model_ver == 'vbrm':\n",
        "            # or args.model_ver == 'vrvb' \\\n",
        "\n",
        "            train_dataset = MedVQAVBClassification(train_folder, train_img_folder, args.dataset_cat, patch_size = args.patch_size, validation=False)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "            val_dataset = MedVQAVBClassification(val_folder, val_img_folder, args.dataset_cat, patch_size = args.patch_size, validation=True)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "        elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = MedVQAGPTClassification(train_folder, train_img_folder, args.dataset_cat, model_ver=args.model_ver, validation=False)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = MedVQAGPTClassification(val_folder, val_img_folder, args.dataset_cat, model_ver=args.model_ver, validation=True)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # num_classes\n",
        "        if args.dataset_cat == 'cat1': args.num_class = 45\n",
        "        elif args.dataset_cat == 'cat2': args.num_class = 16\n",
        "        elif args.dataset_cat == 'cat3': args.num_class = 10\n",
        "\n",
        "    elif args.dataset_type == 'm18':\n",
        "        '''\n",
        "        Train and test dataloader for EndoVis18\n",
        "        '''\n",
        "        # tokenizer\n",
        "        tokenizer = None\n",
        "        if args.tokenizer_ver == 'btv2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-EndoVis-18-VQA/')\n",
        "        elif args.tokenizer_ver == 'btv3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-EndoVis-18-VQA/', do_lower_case=True)\n",
        "        elif args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # data location\n",
        "        train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "        val_seq = [1, 5, 16]\n",
        "        # train_seq = [1, 2, 3, 5, 6, 7, 9, 10, 14, 15, 16]\n",
        "        # val_seq = [4, 11, 12]\n",
        "        folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "        folder_tail = '/vqa/Classification/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'vb' or args.model_ver == 'vbrm' :\n",
        "            # or args.model_ver == 'vrvb' \\\n",
        "\n",
        "            train_dataset = EndoVis18VQAVBClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True)\n",
        "            val_dataset = EndoVis18VQAVBClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False)\n",
        "        elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = EndoVis18VQAGPTClassification(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = EndoVis18VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 18\n",
        "\n",
        "    elif args.dataset_type == 'c80':\n",
        "        '''\n",
        "        Train and test for cholec dataset\n",
        "        '''\n",
        "        # tokenizer\n",
        "        if args.tokenizer_ver == 'btv2': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v2/bert-Cholec80-VQA/')\n",
        "        elif args.tokenizer_ver == 'btv3': tokenizer = BertTokenizer.from_pretrained('./dataset/bertvocab/v3/bert-Cholec80-VQA/', do_lower_case=True)\n",
        "        elif args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # dataloader\n",
        "        train_seq = [1, 2, 3, 4, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40]\n",
        "        val_seq = [5, 11, 12, 17, 19, 26, 27, 31]\n",
        "        folder_head = 'dataset/Cholec80-VQA/Classification/'\n",
        "        folder_tail = '/*.txt'\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'vb' or args.model_ver == 'vbrm':\n",
        "            # or args.model_ver == 'vrvb'\n",
        "\n",
        "            train_dataset = Cholec80VQAVBClassification(train_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=4)\n",
        "            val_dataset = Cholec80VQAVBClassification(val_seq, folder_head, folder_tail, patch_size = args.patch_size)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=2)\n",
        "        elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = Cholec80VQAGPTClassification(train_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = Cholec80VQAGPTClassification(val_seq, folder_head, folder_tail, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 13\n",
        "\n",
        "    elif args.dataset_type == 'psi':\n",
        "        '''\n",
        "        Train and test for psi-ava-vqa dataset\n",
        "        '''\n",
        "        # tokenizer\n",
        "        if args.tokenizer_ver == 'btv2': tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        elif args.tokenizer_ver == 'btv3': tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        elif args.tokenizer_ver == 'gpt2v1':\n",
        "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # dataloader\n",
        "        train_seq  =[\n",
        "                        \"dataset/PSI-AVA-VQA/Train/C1_location.txt\",\n",
        "                        \"dataset/PSI-AVA-VQA/Train/C3_phase.txt\",\n",
        "                        \"dataset/PSI-AVA-VQA/Train/C4_step.txt\"\n",
        "                    ]\n",
        "        val_seq    =[\n",
        "                        \"dataset/PSI-AVA-VQA/Val/C1_location.txt\",\n",
        "                        \"dataset/PSI-AVA-VQA/Val/C3_phase.txt\",\n",
        "                        \"dataset/PSI-AVA-VQA/Val/C4_step.txt\"\n",
        "                    ]\n",
        "\n",
        "        # dataloader\n",
        "        if args.model_ver == 'vb' or args.model_ver == 'vbrm':\n",
        "            # or args.model_ver == 'vrvb'\n",
        "\n",
        "            train_dataset = PSIAVAVQAVBClassification(train_seq, patch_size = args.patch_size)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=4)\n",
        "            val_dataset = PSIAVAVQAVBClassification(val_seq, patch_size = args.patch_size)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=2)\n",
        "        elif args.model_ver == 'efvlegpt2rs18' or args.model_ver == \"efvlegpt2Swin\" or args.model_ver == 'efvlegpt2ViT':\n",
        "\n",
        "            train_dataset = PSIAVAVQAGPTClassification(train_seq, model_ver=args.model_ver)\n",
        "            train_dataloader = DataLoader(dataset=train_dataset, batch_size= args.batch_size, shuffle=True, num_workers=8)\n",
        "            val_dataset = PSIAVAVQAGPTClassification(val_seq, model_ver=args.model_ver)\n",
        "            val_dataloader = DataLoader(dataset=val_dataset, batch_size= args.batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "        # num_classes\n",
        "        args.num_class = 35 #155 #35\n",
        "\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if args.checkpoint is None:\n",
        "        '''visualbert and visualbert resmlp'''\n",
        "        if args.model_ver == 'vb':\n",
        "            model = VisualBertClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class)\n",
        "        elif args.model_ver == 'vbrm':\n",
        "            model = VisualBertResMLPClassification(vocab_size=len(tokenizer), layers=args.encoder_layers, n_heads=args.n_heads, num_class = args.num_class, token_size = int(args.question_len+(args.patch_size * args.patch_size)))\n",
        "        elif args.model_ver == 'efvlegpt2rs18':\n",
        "            model = EFVLEGPT2RS18Classification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "        elif args.model_ver == 'efvlegpt2Swin':\n",
        "            model = EFVLEGPT2SwinClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "        elif args.model_ver == 'efvlegpt2ViT':\n",
        "            model = EFVLEGPT2ViTClassification(num_class = args.num_class, model_subver = args.model_subver, vis_pos_emb = args.vis_pos_emb)\n",
        "\n",
        "        # print(model)\n",
        "        # optimizer\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(args.checkpoint, map_location=str(device))\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_Acc = checkpoint['Acc']\n",
        "        model = checkpoint['model']\n",
        "        optimizer = checkpoint['optimizer']\n",
        "        final_args = checkpoint['final_args']\n",
        "        for key in final_args.keys(): args.__setattr__(key, final_args[key])\n",
        "\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    model = model.to(device)\n",
        "    # print(final_args)\n",
        "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
        "    print('model params: ', pytorch_total_params)\n",
        "    # print(model)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    # validation\n",
        "    if args.validate:\n",
        "        test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=(args.epochs-1), tokenizer = tokenizer, device = device)\n",
        "    else:\n",
        "        for epoch in range(start_epoch, args.epochs):\n",
        "\n",
        "            if epochs_since_improvement > 0 and epochs_since_improvement % 5 == 0:\n",
        "                adjust_learning_rate(optimizer, 0.8)\n",
        "\n",
        "            # train\n",
        "            train_acc = train(args, train_dataloader=train_dataloader, model = model, criterion=criterion, optimizer=optimizer, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            # validation\n",
        "            test_acc, test_c_acc, test_precision, test_recall, test_fscore = validate(args, val_loader=val_dataloader, model = model, criterion=criterion, epoch=epoch, tokenizer = tokenizer, device = device)\n",
        "\n",
        "            if test_acc >= best_results[0]:\n",
        "                epochs_since_improvement = 0\n",
        "\n",
        "                best_results[0] = test_acc\n",
        "                best_epoch[0] = epoch\n",
        "                # print('Best epoch: %d | Best acc: %.6f' %(best_epoch[0], best_results[0]))\n",
        "                save_clf_checkpoint(args.checkpoint_dir, epoch, epochs_since_improvement, model, optimizer, best_results[0], final_args)\n",
        "\n",
        "            else:\n",
        "                epochs_since_improvement += 1\n",
        "                print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "\n",
        "            if train_acc >= 1.0: break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcMJ5Prm0_dL"
      },
      "outputs": [],
      "source": [
        "# Running the training loop with all arguments\n",
        "!python /content/SurgicalGPT/train.py \\\n",
        "--lr=0.00001 \\\n",
        "--checkpoint_dir='checkpoints/efvlegpt2Swin/m18_v1_z_qf_' \\\n",
        "--dataset_type='m18' \\\n",
        "--tokenizer_ver='gpt2v1' \\\n",
        "--model_ver='efvlegpt2Swin' \\\n",
        "--model_subver='v1' \\\n",
        "--vis_pos_emb='zeroes'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0rl37eL7YE2",
        "outputId": "fe78d68e-3281-414a-df8d-558be261ae70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total files: 1560 | Total question: 9014\n",
            "Total files: 447 | Total question: 2769\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data  import DataLoader\n",
        "\n",
        "train_seq = [2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15]\n",
        "val_seq = [1, 5, 16]\n",
        "        # train_seq = [1, 2, 3, 5, 6, 7, 9, 10, 14, 15, 16]\n",
        "        # val_seq = [4, 11, 12]\n",
        "folder_head = '/content/EndoVis-18-VQA/seq_'\n",
        "folder_tail = '/vqa/Classification/*.txt'\n",
        "patch_size = 1\n",
        "batch_size =2\n",
        "\n",
        "train_dataset = EndoVis18VQAVBClassification(train_seq, folder_head, folder_tail, patch_size = patch_size)\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size= batch_size, shuffle=True)\n",
        "val_dataset = EndoVis18VQAVBClassification(val_seq, folder_head, folder_tail, patch_size = patch_size)\n",
        "val_dataloader = DataLoader(dataset=val_dataset, batch_size= batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QsijQlX7gYm"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}